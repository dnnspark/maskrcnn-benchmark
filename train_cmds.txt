from laptop:

python tools/train_net.py --config-file "train_configs/e2e_faster_rcnn_R_50_FPN_1x.yaml"

RuntimeError: CUDA out of memory. Tried to allocate 975.00 MiB (GPU 0; 3.95 GiB total capacity; 2.24 GiB already allocated; 288.06 MiB free; 1.49 MiB cached)

python tools/train_net.py --config-file "train_configs/e2e_faster_rcnn_R_50_FPN_1x.yaml" SOLVER.IMS_PER_BATCH 2 SOLVER.BASE_LR 0.0025 SOLVER.MAX_ITER 720000 SOLVER.STEPS "(480000, 640000)"

RuntimeError: CUDA out of memory. Tried to allocate 49.00 MiB (GPU 0; 3.95 GiB total capacity; 2.43 GiB already allocated; 17.81 MiB free; 81.24 MiB cached)

python tools/train_net.py --config-file "train_configs/e2e_faster_rcnn_R_50_FPN_1x.yaml" SOLVER.IMS_PER_BATCH 1 SOLVER.BASE_LR 0.0025 SOLVER.MAX_ITER 1440000 SOLVER.STEPS "(960000, 1280000)"

** train works! (but with type lag.)


from p3.x16large (8 gpu)

python -m torch.distributed.launch --nproc_per_node=8 tools/train_net.py --config-file "train_configs/e2e_faster_rcnn_R_50_FPN_1x.yaml" OUTPUT_DIR "./out"

** train works!
